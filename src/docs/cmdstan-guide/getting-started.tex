\chapter{Getting Started}

\noindent
This chapter is designed to help users get acquainted with the overall
design of the \Stan language and calling \Stan from the command line.
Later chapters are devoted to expanding on the material in this
chapter with full reference documentation.  The content is identical
to that found on the getting-started with the command-line
documentation on the Stan home page, \url{http://mc-stan.org/}.

\section{For BUGS Users}

An appendix in the language user's guide and reference manual
describes some similarities and important differences between Stan and
BUGS (including WinBUGS, OpenBUGs, and JAGS).


\section{Installation}

For information about supported versions of Windows, Mac and Linux
platforms along with step-by-step installation instructions, see
\refappendix{install}.

\section{Building Stan}

Building Stan itself works the same way across platforms.
To build Stan, first open a command-line terminal application.  Then change
directories to the directory in which Stan is installed (i.e., the
directory containing the file named \code{makefile}).
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cd <stan-home>
\end{Verbatim}
\end{quote}
%
Then make the library with the following make command
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> make bin/libstan.a
\end{Verbatim}
\end{quote}
%
then make the model parser and code generator with the following call,
adjusting the \code{2} in \code{-j2} to the number of CPU cores
available 
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> make -j2 bin/stanc
\end{Verbatim}
\end{quote}
%
On Windows, that'll be \code{bin/stanc.exe}.  

\emph{Warning:} \ The \code{make} program may take 10+ minutes and
consume 2+ GB of memory to build \code{stanc}.  Compiler warnings,
such as \code{uname:~not found}, may be safely ignored.
 
Finally, make the Stan output summary program with the following
make command.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> make bin/print
\end{Verbatim}
\end{quote}
%

Building \code{libstan.a}, \code{bin/stanc}, and \code{bin/print}
needs to be done only once.

\section{Compiling and Executing a Model}\label{compiling-model.section}

The rest of this quick-start guide explains how to code
and run a very simple Bayesian model.

\subsection{A Simple Bernoulli Model}

The following simple model is available in the source
distribution located at \code{<stan-home>} as
%
\begin{quote}
\nolinkurl{src/models/basic_estimators/bernoulli.stan}
\end{quote}
%
The file contains the following model.
%
\begin{quote}
\begin{Verbatim}
data { 
  int<lower=0> N; 
  int<lower=0,upper=1> y[N];
} 
parameters {
  real<lower=0,upper=1> theta;
} 
model {
  theta ~ beta(1,1);
  for (n in 1:N) 
    y[n] ~ bernoulli(theta);
}
\end{Verbatim}
\end{quote}
%
The model assumes the binary observed data \code{y[1],...,y[N]}
are i.i.d.\ with Bernoulli chance-of-success \code{theta}.  The
prior on \code{theta} is \code{beta(1,1)} (i.e., uniform).

\subsubsection{Implicit Uniform Priors}

If no prior is specified for a parameter, it is implicitly given a
uniform prior on its support.  For parameters such as \code{theta} in
the example, which are constrained to fall between 0 and 1, this
produces a proper uniform distribution on the support of \code{theta}.
Because $\distro{Beta}(1,1)$ is the uniform distribution, the
following sampling statement can be eliminated from the model without
changing the log probability calculation.
%
\begin{quote}
\begin{Verbatim}
  theta ~ beta(1,1);
\end{Verbatim}
\end{quote}

For parameters with unbounded support, the implicit uniform prior is
improper.  Stan allows improper priors to be specified in models, but
posteriors must be proper in order for sampling to succeed.


\subsubsection{Constraints on Parameters}

The variable \code{theta} is defined with lower and upper bounds,
which constrain its value.  Parameters with constrained support should
always specify appropriate constraints in the parameter declaration;
if the constraints are absent, sampling will either slow down or stop
altogether based on whether the initial values satisfy the constraints.

\subsubsection{Vectorizing Sampling Statements}

Iterations of the model will be faster if the loop over sampling
statements is \textit{vectorized} by replacing
%
\begin{quote}
\begin{Verbatim}
  for (n in 1:N) 
    y[n] ~ bernoulli(theta);
\end{Verbatim}
\end{quote}
%
with the equivalent vectorized form,
%
\begin{quote}
\begin{Verbatim}
  y ~ bernoulli(theta);
\end{Verbatim}
\end{quote}
%
Performance gains from vectorization are not because loops are slow in
Stan, but because calls to sampling statements are slow.
Vectorization allows multiple calls to a sampling statement to be
replaced with a single call that can share common calculations for the
log probability function, its gradients, and error checking.  For more
tips on optimizing the performance of Stan models, see the chapter in
the language user's guide and reference manual on optimization.


\subsection{Data Set}

A data set of $\mbox{\code{N}}=10$ observations is available in the file
%
\begin{quote}
\nolinkurl{src/models/basic_estimators/bernoulli.data.R}
\end{quote}
%
The content of the file is as follows.
%
\begin{quote}
\begin{Verbatim}
N <- 10
y <- c(0,1,0,0,0,0,0,0,0,1)
\end{Verbatim}
\end{quote}
%
This defines the contents of two variables, \code{N} and \code{y},
using an R-like syntax (see \refchapter{dump} for more information).



\subsection{Generating and Compiling the Model}

A single call to \code{make} will generate the \Cpp code for a
model with a name ending in \code{.stan} and compile it for
execution.  This call will also compile the library \code{libstan.a}
and the parser/code generator \code{stanc} if they have not already
been compiled.

First, change directories to \code{<stan-home>}, the directory where
Stan was unpacked that contains the file named \code{makefile} and
a subdirectory called \code{src/}.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cd <stan-home>
\end{Verbatim}
\end{quote}
%
Then issue the following command:
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> make src/models/basic_estimators/bernoulli 
\end{Verbatim}
\end{quote}
%
The command for Windows is the same, including the forward slashes.

The \code{make} command may be applied to files in locations
that are not subdirectories issued from another directory as follows.
Just replace the relative path \code{src/models/...} with the
actual path.

The C++ generated for the model and its compiled executable
form will be placed in the same directory as the model.


\subsection{Sampling from the Model}

The model can be executed from the directory in which it resides.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cd src/models/basic_estimators 
\end{Verbatim}
\end{quote}
%
To execute sampling of the model under Linux or Mac, use
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bernoulli sample data file=bernoulli.data.R
\end{Verbatim}
\end{quote}
%
The \code{./} prefix before the executable is only required under
Linux and the Mac when executing a model from the directory in which
it resides.

For the Windows DOS terminal, the \code{./} prefix is not needed,
resulting in the following command.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> bernoulli sample data file=bernoulli.data.R
\end{Verbatim}
\end{quote}
%
Whether the command is run in Windows, Linux, or on the Mac, the
output is the same.  First, the parameters are echoed to the standard output,
which shows up on the terminal as follows.
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
 method = sample (Default)
   sample
     num_samples = 1000 (Default)
     num_warmup = 1000 (Default)
     save_warmup = 0 (Default)
     thin = 1 (Default)
     adapt
       engaged = 1 (Default)
       gamma = 0.050000000000000003 (Default)
       delta = 0.80000000000000004 (Default)
       kappa = 0.75 (Default)
       t0 = 10 (Default)
       init_buffer = 75 (Default)
       term_buffer = 50 (Default)
       window = 25 (Default)
     algorithm = hmc (Default)
       hmc
         engine = nuts (Default)
           nuts
             max_depth = 10 (Default)
         metric = diag_e (Default)
         stepsize = 1 (Default)
         stepsize_jitter = 0 (Default)
 id = 0 (Default)
 data
   file = bernoulli.data.R
 init = 2 (Default)
 random
   seed = 4294967295 (Default)
 output
   file = output.csv (Default)
   diagnostic_file =  (Default)
   refresh = 100 (Default)
...
\end{Verbatim}
\end{quote}
%
The ellipses (\code{...}) indicate that the output continues (as
described below).

After the configuration has been displayed a short timing warning
is given.
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
...
Gradient evaluation took 4e-06 seconds
1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Adjust your expectations accordingly!
...
\end{Verbatim}
\end{quote}

Next, the sampler counts up the iterations in place, reporting
percentage completed, ending as follows.
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
...
Iteration:    1 / 2000 [  0%]  (Warmup)
...
Iteration: 1000 / 2000 [ 50%]  (Warmup)
Iteration: 1001 / 2000 [ 50%]  (Sampling)
...
Iteration: 2000 / 2000 [100%]  (Sampling)
...
\end{Verbatim}
\end{quote}

\subsubsection{Sampler Output}

Each execution of the model results in the samples from a single
Markov chain being written to a file in comma-separated value (CSV) format.
The default name of the output file is \nolinkurl{output.csv}.

The first part of the output file just repeats the parameters
as comments (i.e., lines beginning with the pound sign (\Verb|#|)).
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
# stan_version_major = 2
# stan_version_minor = 1
# stan_version_patch = 0
# model = bernoulli_model
# method = sample (Default)
#   sample
#     num_samples = 1000 (Default)
#     num_warmup = 1000 (Default)
#     save_warmup = 0 (Default)
#     thin = 1 (Default)
#     adapt
#       engaged = 1 (Default)
#       gamma = 0.050000000000000003 (Default)
#       delta = 0.80000000000000004 (Default)
#       kappa = 0.75 (Default)
#       t0 = 10 (Default)
#       init_buffer = 75 (Default)
#       term_buffer = 50 (Default)
#       window = 25 (Default)
#     algorithm = hmc (Default)
#       hmc
#         engine = nuts (Default)
#           nuts
#             max_depth = 10 (Default)
#         metric = diag_e (Default)
#         stepsize = 1 (Default)
#         stepsize_jitter = 0 (Default)
# id = 0 (Default)
# data
#   file = bernoulli.data.R
# init = 2 (Default)
# random
#   seed = 355899897
# output
#   file = output.csv (Default)
#   diagnostic_file =  (Default)
#   refresh = 100 (Default)
...
\end{Verbatim}
\end{quote}
%
This is then followed by a header indicating the
names of the values sampled.
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
...
lp__,accept_stat__,stepsize__,treedepth__,n_leapfrog__,n_divergent__,theta
...
\end{Verbatim}
\end{quote}
%
The first column gives the log probability.  The next columns, here
columns two through five, provide sampler-dependent information. For
basic Hamiltonian Monte Carlo (HMC) and its adaptive variant No-U-Turn
sampler (NUTS), the sampler-depedent parameters are described in the
following table.
%
\begin{center}
\begin{tabular}{l|l|l}
{\it Sampler} & {\it Parameter} & {\it Description} 
\\ \hline \hline
\HMC & \Verb| accept_stat__ | &  Metropolis acceptance probability
\\
\HMC & \Verb| stepsize__  | & Integrator step size
\\
\HMC & \Verb| int_time__  | & Total integration time
\\
\NUTS & \Verb| accept_stat__  | & Metropolis acceptance probability
\\
& & averaged over samples in the slice 
\\
\NUTS & \Verb| stepsize__ | & Integrator step size
\\
\NUTS & \Verb| treedepth__  | & Tree depth
\\
\NUTS & \Verb| n_leapfrog__  | & Number of leapfrog calculations
\\
\NUTS & \Verb| n_divergent__  | & Number of divergent iterations
\\
\end{tabular}
\end{center}
%
The rest of the columns in the header correspond to model parameters, here
just \code{theta} in the sixth column.  The parameter name header is
output before warmup begins.

The result of any adaptation taking place during warmup is output next
after the parameter names.  
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
...
# Adaptation terminated
# Step size = 1.81311
# Diagonal elements of inverse mass matrix:
# 0.415719
...
\end{Verbatim}
\end{quote}
%
The default sampler is NUTS with an adapted step size and a diagonal
inverse mass matrix.  For the running example, the step size is
1.81311, and the inverse mass contains the single entry 0.415719
corresponding to the parameter \code{theta}.

Samples from each iteration are printed out next, one per line in
columns corresponding to the headers.
%
\footnote{There are repeated entries due to the Metropolis accept step
in the No-U-Turn sampling algorithm.}
%
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
...
-6.95293,0.945991,1.09068,2,3,0.335074
-6.92373,0.938744,1.09068,1,1,0.181194
-6.83655,0.934833,1.09068,2,3,0.304882
...
-7.01732,1,1.09068,1,1,0.348244
-8.96652,0.48441,1.09068,1,1,0.549066
-7.22574,1,1.09068,1,1,0.383089
\end{Verbatim}
\end{quote}
%

The output ends with timing details,%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
...
#  Elapsed Time: 0.006811 seconds (Warm-up)
#                0.011645 seconds (Sampling)
#                0.018456 seconds (Total)
\end{Verbatim}
\end{quote}


\subsubsection{Summarizing Sampler Output}

The command-line program \code{bin/print} will display summary
information about the run (for more information, see
\refchapter{print-command}). To run \code{print} on the output file
generated for \code{bernoulli} on Linux or Mac,  use
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> <stan-home>/bin/print output.csv
\end{Verbatim}
\end{quote}
%
where \code{<stan-home>} is the path to where Stan was unpacked.  For
Windows use backslashes for the executable,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> <stan-home>\bin\print output.csv
\end{Verbatim}
\end{quote}
%
The output of the command will display information about the run
followed by information for each parameter and generated quantity. For
\code{bernoulli}, we ran 1 chain and saved 1000 iterations. The
information is echoed to the standard output stream.  For the running
example, the path to \code{<stan-home>} can be specified from the
directory in which the Bernoulli model resides using \code{../} (with
backslashes on Windows) as
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ../../../bin/print output.csv 
\end{Verbatim}
\end{quote}
%
For Windows, reverse the slashes.  The output is
%
\begin{Verbatim}[fontshape=sl,fontsize=\footnotesize]
Inference for Stan model: bernoulli_model
1 chains: each with iter=(1000); warmup=(0); thin=(1); 1000 iterations saved.

Warmup took (0.0066) seconds, 0.0066 seconds total
Sampling took (0.011) seconds, 0.011 seconds total

                 Mean     MCSE   StdDev        5%   50%   95%  N_Eff  N_Eff/s  R_hat
lp__             -7.3  3.5e-02  6.9e-01  -8.7e+00  -7.0  -6.7    390    34020   1.00
accept_stat__    0.64  1.2e-02  3.6e-01   5.1e-03  0.74   1.0    882    76898   1.00
stepsize__        1.8  7.8e-15  5.6e-15   1.8e+00   1.8   1.8   0.50       44   1.00
treedepth__     0.076  8.6e-03  2.7e-01   0.0e+00  0.00   1.0    942    82167   1.00
n_leapfrog__     2.7  4.9e-02  1.3e+00    1.0   3.0   3.0    716    65090  1.0e+00
n_divergent__    0.00  0.0e+00  0.0e+00   0.0e+00  0.00  0.00   1000    90909   1.00
theta            0.25  4.2e-03  1.2e-01   9.0e-02  0.23  0.47    827    72146   1.00

Samples were drawn using hmc with nuts.
For each parameter, N_Eff is a crude measure of effective sample size,
and R_hat is the potential scale reduction factor on split chains (at 
convergence, R_hat=1).
\end{Verbatim}
%
In addition to the general information about the runs, \code{print}
displays summary statistics for each parameter and generated
quantity.

In the \code{bernoulli} model, there is a single parameter,
\code{theta}. The mean, standard error of the mean, standard
deviation, the 5\%, 50\%, and 95\% quantiles, number of effective
samples (total and per second), and $\hat{R}$ value are displayed.
These quantities and their uses are described in detail in the
introductory Markov chain Monte Carlo (MCMC) chapter of the language
user's guide and reference manual.

The command \code{bin/print} can be called with more than one csv file
by separating filenames with spaces. It will also take wildcards in
specifying filenames. A typical usage of Stan from the command line
would first create one or more Markov chains by calling the model
executable, typically in parallel, writing the output CSV file for
each into its own directory.  Next, after all of the processes are
finished, the results would be analyzed using \code{print} to assess
convergence and inspect the means and quantiles of the fitted
variables.  Additionally, downstream inferences may be performed using
the samples (e.g., to make decisions or predictions for unseen data).

\subsection{Compile-time and Run-time Warnings}

Stan tries to report warnings in order to help users correctly
formulate and ebug models.  There are two warnings in particular that
deserve further explanation up front because we have not been able to
formulate wording clearly enough to prevent confusion.

\subsubsection{Metropolis Rejection Warning}

The first problematic warning message involves the Metropolis sampler
rejecting a proposal due to an error arising in the evaluation of the
log probability function.  Such errors are typically due to underflow
or overflow in numerical computations that are the unavoidable
consequence of floating-point approximations to continuous real
values.  The following is an example of such a message.
%
\begin{quote}\small
\begin{Verbatim}
Informational Message: The current Metropolis proposal is about
to be rejected because of the following issue:
  Error in function stan::prob::normal_log(N4stan5agrad3varE): 
  Scale parameter is 0:0, but must be > 0!
If this warning occurs sporadically, such as for highly constrained
variable types like covariance matrices, then the sampler is fine, 
but if this warning occurs often then the model may be either severely 
ill-conditioned or misspecified.
\end{Verbatim}
\end{quote}
%
Despite using the word ``Error'' in the embedded report of the
numerical issue, this is just an \emph{informational message} \,
(i.e., a warning); it is \emph{not} \, an error.  Particularly in
early stages of sampler adaptation before adaptation has converged on
the high-mass volume of the posterior, the numerical approximation to
functions and to the Hamiltonian dynamics followed by the sampler can
lead to numerical issues.  As the message tries to indicate, if the
message only occurs sporadically, then {\it the sampler is fine} and
the user need not worry.  In particular, the post-adaptation draws are
still valid.

The reason this message is presented at all is that if it occurs
repeatedly, the model may have a poorly conditioned constraint
(typically with covariance or correlation matrices) or may be
misformulated.  In a future version of Stan, we plan to rank such
messages by severity and turn this one off by default so as not to
needlessly worry users.

\subsubsection{Jacobian Required Warning}

The second problematic warning message appears when a model is
compiled and involves the requirement of a Jacobian adjustment to the
log probability.  If the left-hand side of a sampling statement
involves a \emph{non-linear}\ transform, then a Jacobian adjustment
must be made.  A chapter of the language user's guide and reference
manual include a full description of the Jacobians that are
automatically applied and how to define Jacobians for transforms;  in
the user-defined distribution chapter there are examples.

In linear transforms or matrix/array slicing, spurious warnings
arise.  An example is the following model, which involves extraction
of a row from a matrix for vectorized sampling.
%
\begin{quote}
\begin{Verbatim}
parameters {
  matrix[20,10] y;
}
model {
  for (m in 1:20)
    row(y,m) ~ normal(0,1);
}
\end{Verbatim}
\end{quote}
%
Compiling the above model leads to the following spurious warning.
%
\begin{quote}\small
\begin{Verbatim}
Warning (non-fatal):  sampling statement (~) contains a transformed
  parameter or local variable. You must increment lp__ with the log 
  absolute determinant of the Jacobian of the transform.
     Sampling Statement left-hand-side expression:
          row(y,m) ~ normal_log(...)
\end{Verbatim}
\end{quote}
%
Such messages may be ignored if the transform involves is linear or
only involves pulling out slices or blocks of larger structures.

\subsection{Optimization}

Stan can be used for finding posterior modes as well as sampling from
the posterior.   The model does not need to be recompiled in order to
switch from optimization to sampling, and the data input format is the
same.  Although many command-line arguments may be provided to
configure the optimizer, the following minimal command suffices, using
defaults for everything but where to find the data file.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
./bernoulli optimize data file=bernoulli.data.R
\end{Verbatim}
\end{quote}
%
which prints out
%
\begin{quote}
\begin{Verbatim}[fontsize=\footnotesize]
 method = optimize
   optimize
     algorithm = bfgs (Default)
       bfgs
         init_alpha = 0.001 (Default)
         tol_obj = 1e-08 (Default)
         tol_grad = 1e-08 (Default)
         tol_param = 1e-08 (Default)
     iter = 2000 (Default)
     save_iterations = 0 (Default)
 id = 0 (Default)
 data
   file = bernoulli.data.R
 init = 2 (Default)
 random
   seed = 2907588507
 output
   file = output.csv (Default)
   append_sample = 0 (Default)
   diagnostic_file =  (Default)
   append_diagnostic = 0 (Default)
   refresh = 100 (Default)

initial log joint probability = -10.9308
    Iter      log prob        ||dx||      ||grad||    alpha  # evals  Notes 
       7      -5.00402   3.67055e-07   3.06339e-11        1       10   
Optimization terminated normally: 
  Convergence detected: change in objective function was below
  tolerance
\end{Verbatim}
\end{quote}
%
The first part of the output reports on the configuration used, here
indicating the default BFGS optimizer, with default initial stepsize
and tolerances for monitoring convergence.  The second part of the
output indicates how well the algorithm fared, here converging and
terminating normally.  The numbers reported indicate that it took 7
iterations and 10 gradient evaluations, resulting in a final state
state where the change in parameters was roughly 3.7e-7 and the length
of the gradient roughly 3e-11.  The \code{alpha} value is for step
size used.  This is, not surprisingly, far fewer iterations than
required for sampling; even fewer iterations would be used with less
stringent user-specified convergence tolerances.


\subsubsection{Output from Optimization}

The output from optimization is written into the file
\code{output.csv} by default.  The output follows the same pattern as the
output for sampling, first dumping the entire set of parameters used.
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
# stan_version_major = 2
# stan_version_minor = 1
# stan_version_patch = 0
# model = bernoulli_model
# method = optimize
#   optimize
#     algorithm = bfgs (Default)
#       bfgs
#         init_alpha = 0.001 (Default)
#         tol_obj = 1e-08 (Default)
#         tol_grad = 1e-08 (Default)
#         tol_param = 1e-08 (Default)
#     iter = 2000 (Default)
#     save_iterations = 0 (Default)
# id = 0 (Default)
# data
#   file = bernoulli.data.R
# init = 2 (Default)
# random
#   seed = 2907588507
# output
#   file = output.csv (Default)
#   append_sample = 0 (Default)
#   diagnostic_file =  (Default)
#   append_diagnostic = 0 (Default)
#   refresh = 100 (Default)
lp__,theta
-5.00402,0.2000000000030634
\end{Verbatim}
\end{quote}
%
Note that everything is a comment other than a line for the header,
and a line for the values.  Here, the header indicates the
unnormalized log probability with \code{lp\_\_} and the model
parameter \code{theta}.  The maximum log probability is -5.0 and the
posterior mode for \code{theta} is 0.20.  The mode exactly matches
what we would expect from the data.  
%
\footnote{The Jacobian adjustment included for the sampler's log
  probability function is not applied during optimization, because it can
  change the shape of the posterior and hence the solution.}
%
Because the prior was uniform, the result 0.20 represents the maximum
likelihood estimate (MLE) for the very simple Bernoulli model.  Note
that no uncertainty is reported.  


\subsection{Configuring Command-Line Options}

The command-line options for running a model are detailed in
\refchapter{stan-cmd}. They can also be printed on the command line
using Linux or Mac OS with
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bernoulli help-all
\end{Verbatim}
\end{quote}
%
and on Windows with
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> bernoulli help-all
\end{Verbatim}
\end{quote}
%
It may help to glance at the command-line skeletons in
\reffigure{nuts-command} through \reffigure{diagnostic-command} to get
a handle on the options then read the detailed descriptions earlier in
\refchapter{stan-cmd}.

\subsection{Testing Stan}\label{testing-stan.section}

To run the Stan unit tests of basic functionality, run the following
commands from a shell (where \code{<stan-home>} is replaced top-level
directory into which Stan was unpacked; it should contain a file named
\code{makefile}).%
%
\footnote{Although command-line Stan runs with earlier versions of
  \code{make}, the unit tests require version 3.81 or higher; see
  \refsection{windows-make-install} for installation instructions.}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cd <stan-home>
> make -j4 O=0 test-headers
> make -j4 O=0 src/test/unit
> make -j4 O=0 src/test/unit-agrad-rev
> make -j4 O=0 src/test/unit-agrad-fwd
> make -j4 O=0 src/test/unit-distribution
> make -j4 O=3 src/test/CmdStan/models
\end{Verbatim}
\end{quote}
%
As before, \code{-j4} indicates that four processes should be run in
parallel; adjust the value \code{4} to correspond to the number of CPU
cores available. Code optimization is specified by the letter `O'
followed by an equal sign followed by the digit `0' for no
optimization and `3' for more optimization; optimization slows down
compilation of the executable but reduces its execution time.
Warnings can be safely ignored if the tests complete without a
\code{FAIL} error.

\emph{Warning}: \ The unit tests can take 30+ minutes and consume 3+
GB of memory with the default compiler, \code{g++}.  The distribution
test and model tests can take even longer.  It is faster to run the
Clang compiler (option \code{CC=clang++}), and to run in multiple
processes in parallel (e.g., option \code{-j4} for four threads).



